# ğŸ“ Educational Benchmarks & LLM-Evaluation Suite

A ğŸš€ **single-stop repository** for evaluating **GPT-family**, **LLaMA**, **Gemini**, and other large-language models on standardized tests such as **GRE**, **GMAT**, **SAT**, **TOEFL**, and **IELTS**.

## ğŸ“¦ Features

* ğŸ“ **Curated JSON datasets** for each exam section
* ğŸ One dedicated **Python driver per dataset per model**
* ğŸ“Š Generic summarizer to convert JSON files into latency/accuracy CSV tables
* ğŸ“ˆ Utility scripts for cross-dataset accuracy aggregation and plotting

---

## ğŸ“‚ Repository Layout

### ğŸ“Œ Folder Naming Rules

* `GRE*`: Math, RC, Verbal
* `GMAT/`: Quant, Verbal, Data Insights
* `SAT/`: Math & Reading
* `TOFEL/`: Reading & Listening
* `IELTS/`: Reading & Listening

---

## ğŸ“ Question Type Examples

![Question Type Examples](https://raw.githubusercontent.com/ltang24/Education/main/example.jpg)

## ğŸ› ï¸ Prerequisites

* ğŸ Python â‰¥ 3.10

---

## âš¡ Quick Start

```bash
pip install g4f

# ğŸ“¥ Clone the repository
git clone https://github.com/<your-org>/Education.git
cd Education

# ğŸŒ± Set up virtual environment
python -m venv .venv && source .venv/bin/activate

# â–¶ï¸ Run dataset example: GRE Math Medium 
cd "GRE Math Medium"
python run_gpt4o.py   # âœ gre_math_medium_gpt4o.json (includes overall accuracy)

# ğŸ“‹ Generate Latency CSV
cd ../../tools
python runtime.py ../GRE\ Math\ Medium/gre_math_medium_gpt4o.json \
                  --out ../runtime_gre/gre_math_medium_gpt4o.csv
```

---

## ğŸ¤ Contributing

Your contributions are always welcome! Please:

* Fork ğŸ´
* Branch ğŸŒ¿ (`git checkout -b feature/new-feature`)
* Commit ğŸ’¾ (`git commit -m 'Add awesome feature'`)
* Push ğŸš€ (`git push origin feature/new-feature`)
* Pull Request ğŸ‰

---

## ğŸ–¼ï¸ Visual Breakdown Illustration

<img src="breakdown.jpg" alt="Breakdown Overview" width="100%"/>

## ğŸ“œ License

Distributed under the MIT License ğŸ“„. See [`LICENSE`](LICENSE) for more information.
