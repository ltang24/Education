{
    "strategy": "3-shot",
    "accuracy": 0.62,
    "total_questions": 50,
    "correct_count": 31,
    "details": [
        {
            "question_number": 1,
            "expected": [
                "detection",
                "an unambiguous"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "detection",
                "an unambiguous"
            ],
            "model_explanation": "Final Answer: detection, an unambiguous",
            "model_response": "Final Answer: detection, an unambiguous",
            "runtime": 7.91,
            "correct": true
        },
        {
            "question_number": 2,
            "expected": [
                "sterling reputation of",
                "exhaustive"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "sterling reputation of",
                "exhaustive"
            ],
            "model_explanation": "",
            "model_response": "sterling reputation of, exhaustive",
            "runtime": 0.34,
            "correct": true
        },
        {
            "question_number": 3,
            "expected": [
                "feeble",
                "robust"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "feeble",
                "robust"
            ],
            "model_explanation": "Final Answer: feeble, robust",
            "model_response": "Final Answer: feeble, robust",
            "runtime": 4.72,
            "correct": true
        },
        {
            "question_number": 4,
            "expected": [
                "efficiently",
                "vacant"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "efficiently",
                "vacant"
            ],
            "model_explanation": "",
            "model_response": "efficiently, vacant",
            "runtime": 5.72,
            "correct": true
        },
        {
            "question_number": 5,
            "expected": [
                "limitation",
                "objectivity"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "limitation",
                "objectivity"
            ],
            "model_explanation": "Final Answer: limitation, objectivity",
            "model_response": "Final Answer: limitation, objectivity",
            "runtime": 6.21,
            "correct": true
        },
        {
            "question_number": 6,
            "expected": [
                "favor",
                "loath"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "favor",
                "willing"
            ],
            "model_explanation": "",
            "model_response": "favor, willing",
            "runtime": 0.56,
            "correct": false
        },
        {
            "question_number": 7,
            "expected": [
                "decisive",
                "vacillation"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "compassionate",
                "vacillation"
            ],
            "model_explanation": "",
            "model_response": "compassionate, vacillation",
            "runtime": 6.96,
            "correct": false
        },
        {
            "question_number": 8,
            "expected": [
                "delightful",
                "contagious"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "delightful",
                "contagious"
            ],
            "model_explanation": "",
            "model_response": "delightful, contagious",
            "runtime": 4.98,
            "correct": true
        },
        {
            "question_number": 9,
            "expected": [
                "aimlessness",
                "discursions"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "flippancy",
                "subtlety"
            ],
            "model_explanation": "Final Answer: flippancy, subtlety",
            "model_response": "Final Answer: flippancy, subtlety",
            "runtime": 4.95,
            "correct": false
        },
        {
            "question_number": 10,
            "expected": [
                "inversely related",
                "unfeeling"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "mutually reinforcing",
                "unfeeling"
            ],
            "model_explanation": "",
            "model_response": "mutually reinforcing, unfeeling",
            "runtime": 6.69,
            "correct": false
        },
        {
            "question_number": 11,
            "expected": [
                "an anomalous",
                "embodies"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "an anomalous",
                "embodies"
            ],
            "model_explanation": "Final Answer: an anomalous, embodies",
            "model_response": "Final Answer: an anomalous, embodies",
            "runtime": 7.78,
            "correct": true
        },
        {
            "question_number": 12,
            "expected": [
                "impetuous",
                "circumspect"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "impetuous",
                "circumspect"
            ],
            "model_explanation": "Final Answer: impetuous, circumspect",
            "model_response": "Final Answer: impetuous, circumspect",
            "runtime": 6.79,
            "correct": true
        },
        {
            "question_number": 13,
            "expected": [
                "virtue",
                "derive from"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "drawback",
                "derive from"
            ],
            "model_explanation": "",
            "model_response": "drawback, derive from",
            "runtime": 10.56,
            "correct": false
        },
        {
            "question_number": 14,
            "expected": [
                "persistent",
                "corruption"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "persistent",
                "corruption"
            ],
            "model_explanation": "Final Answer: persistent, corruption",
            "model_response": "Final Answer: persistent, corruption",
            "runtime": 7.63,
            "correct": true
        },
        {
            "question_number": 15,
            "expected": [
                "nearly infinite",
                "eliminated"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "nearly infinite",
                "eliminated"
            ],
            "model_explanation": "Final Answer: nearly infinite, eliminated",
            "model_response": "Final Answer: nearly infinite, eliminated",
            "runtime": 6.97,
            "correct": true
        },
        {
            "question_number": 16,
            "expected": [
                "erudition",
                "scarce"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "erudition",
                "scarce"
            ],
            "model_explanation": "Final Answer: erudition, scarce",
            "model_response": "Final Answer: erudition, scarce",
            "runtime": 4.98,
            "correct": true
        },
        {
            "question_number": 17,
            "expected": [
                "precipitate",
                "forbore"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "tactful",
                "forbore"
            ],
            "model_explanation": "Final Answer: tactful, forbore",
            "model_response": "Final Answer: tactful, forbore",
            "runtime": 6.01,
            "correct": false
        },
        {
            "question_number": 18,
            "expected": [
                "too harshly judged",
                "judicious"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "frankly acknowledged",
                "judicious"
            ],
            "model_explanation": "",
            "model_response": "frankly acknowledged, judicious",
            "runtime": 6.16,
            "correct": false
        },
        {
            "question_number": 19,
            "expected": [
                "abundance",
                "rapacious depletion"
            ],
            "models_tried": [
                "gpt-4o",
                "gpt-4"
            ],
            "model_used": "gpt-4",
            "model_answer": [
                "abundance",
                "rapacious depletion"
            ],
            "model_explanation": "",
            "model_response": "abundance, rapacious depletion",
            "runtime": 3.06,
            "correct": true
        },
        {
            "question_number": 20,
            "expected": [
                "neglects",
                "problematic"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "neglects",
                "problematic"
            ],
            "model_explanation": "Final Answer: neglects, problematic",
            "model_response": "Final Answer: neglects, problematic",
            "runtime": 8.72,
            "correct": true
        },
        {
            "question_number": 21,
            "expected": [
                "ancestry",
                "obvious"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o-mini",
            "model_answer": [],
            "model_explanation": "",
            "model_response": "",
            "runtime": 6.27,
            "correct": false
        },
        {
            "question_number": 22,
            "expected": [
                "derivative",
                "remarkably pioneering"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "derivative",
                "remarkably pioneering"
            ],
            "model_explanation": "",
            "model_response": "derivative, remarkably pioneering",
            "runtime": 5.49,
            "correct": true
        },
        {
            "question_number": 23,
            "expected": [
                "clandestine",
                "open"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "clandestine",
                "open"
            ],
            "model_explanation": "",
            "model_response": "clandestine, open",
            "runtime": 7.05,
            "correct": true
        },
        {
            "question_number": 24,
            "expected": [
                "churlish",
                "admire"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "churlish",
                "admire"
            ],
            "model_explanation": "",
            "model_response": "churlish, admire",
            "runtime": 7.24,
            "correct": true
        },
        {
            "question_number": 25,
            "expected": [
                "arcane",
                "obscure"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "arcane",
                "obscure"
            ],
            "model_explanation": "Final Answer: arcane, obscure",
            "model_response": "Final Answer: arcane, obscure",
            "runtime": 8.55,
            "correct": true
        },
        {
            "question_number": 26,
            "expected": [
                "unethical",
                "skullduggery"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "unethical",
                "skullduggery"
            ],
            "model_explanation": "Final Answer: unethical, skullduggery",
            "model_response": "Final Answer: unethical, skullduggery",
            "runtime": 5.91,
            "correct": true
        },
        {
            "question_number": 27,
            "expected": [
                "fervor",
                "indiscriminate"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "fervor",
                "indiscriminate"
            ],
            "model_explanation": "",
            "model_response": "fervor, indiscriminate",
            "runtime": 6.17,
            "correct": true
        },
        {
            "question_number": 28,
            "expected": [
                "challenged",
                "maintaining"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "challenged",
                "maintaining"
            ],
            "model_explanation": "Final Answer: challenged, maintaining",
            "model_response": "Final Answer: challenged, maintaining",
            "runtime": 5.77,
            "correct": true
        },
        {
            "question_number": 29,
            "expected": [
                "naysayer",
                "absorbing"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "naysayer",
                "perplexing"
            ],
            "model_explanation": "Final Answer: naysayer, perplexing",
            "model_response": "Final Answer: naysayer, perplexing",
            "runtime": 7.86,
            "correct": false
        },
        {
            "question_number": 30,
            "expected": [
                "complicated by",
                "obscured"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "complicated by",
                "obscured"
            ],
            "model_explanation": "Final Answer: complicated by, obscured",
            "model_response": "Final Answer: complicated by, obscured",
            "runtime": 7.33,
            "correct": true
        },
        {
            "question_number": 31,
            "expected": [
                "limits",
                "lay outside"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "limits",
                "lay outside"
            ],
            "model_explanation": "Final Answer: limits, lay outside",
            "model_response": "Final Answer: limits, lay outside",
            "runtime": 8.25,
            "correct": true
        },
        {
            "question_number": 32,
            "expected": [
                "interpreting",
                "education"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "interpreting",
                "education"
            ],
            "model_explanation": "Final Answer: interpreting, education",
            "model_response": "Final Answer: interpreting, education",
            "runtime": 6.31,
            "correct": true
        },
        {
            "question_number": 33,
            "expected": [
                "mercurial",
                "inconstancy"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "importunate",
                "inconstancy"
            ],
            "model_explanation": "",
            "model_response": "importunate, inconstancy",
            "runtime": 6.91,
            "correct": false
        },
        {
            "question_number": 34,
            "expected": [
                "laboratory",
                "tried"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "laboratory",
                "tried"
            ],
            "model_explanation": "",
            "model_response": "laboratory, tried",
            "runtime": 5.53,
            "correct": true
        },
        {
            "question_number": 35,
            "expected": [
                "generated in",
                "never leaves"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "trapped in",
                "never leaves"
            ],
            "model_explanation": "",
            "model_response": "trapped in, never leaves",
            "runtime": 7.35,
            "correct": false
        },
        {
            "question_number": 36,
            "expected": [
                "intransigent",
                "uncompromising"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "munificent",
                "uncompromising"
            ],
            "model_explanation": "Final Answer: munificent, uncompromising",
            "model_response": "Final Answer: munificent, uncompromising",
            "runtime": 9.67,
            "correct": false
        },
        {
            "question_number": 37,
            "expected": [
                "slowed",
                "drag on"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "slowed",
                "drag on"
            ],
            "model_explanation": "Final Answer: slowed, drag on",
            "model_response": "Final Answer: slowed, drag on",
            "runtime": 9.32,
            "correct": true
        },
        {
            "question_number": 38,
            "expected": [
                "familiar",
                "revisionist"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "familiar",
                "revisionist"
            ],
            "model_explanation": "Final Answer: familiar, revisionist",
            "model_response": "Final Answer: familiar, revisionist",
            "runtime": 6.82,
            "correct": true
        },
        {
            "question_number": 39,
            "expected": [
                "trivial",
                "importance"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "trivial",
                "banality"
            ],
            "model_explanation": "",
            "model_response": "trivial, banality",
            "runtime": 4.76,
            "correct": false
        },
        {
            "question_number": 40,
            "expected": [
                "obeisance",
                "mixed"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "clarity",
                "mixed"
            ],
            "model_explanation": "Final Answer: clarity, mixed",
            "model_response": "Final Answer: clarity, mixed",
            "runtime": 8.75,
            "correct": false
        },
        {
            "question_number": 41,
            "expected": [
                "wide-ranging",
                "mimic"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [],
            "model_explanation": "",
            "model_response": "",
            "runtime": 13.1,
            "correct": false
        },
        {
            "question_number": 42,
            "expected": [
                "decline",
                "health"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [],
            "model_explanation": "",
            "model_response": "",
            "runtime": 8.43,
            "correct": false
        },
        {
            "question_number": 43,
            "expected": [
                "an anonymous",
                "confessional"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "evenhanded",
                "confessional"
            ],
            "model_explanation": "",
            "model_response": "evenhanded, confessional",
            "runtime": 6.18,
            "correct": false
        },
        {
            "question_number": 44,
            "expected": [
                "secondhand testimony",
                "anecdote"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "long-term observation",
                "anecdote"
            ],
            "model_explanation": "",
            "model_response": "long-term observation, anecdote",
            "runtime": 5.79,
            "correct": false
        },
        {
            "question_number": 45,
            "expected": [
                "relish",
                "insufferable"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "relish",
                "insufferable"
            ],
            "model_explanation": "Final Answer: relish, insufferable",
            "model_response": "Final Answer: relish, insufferable",
            "runtime": 6.46,
            "correct": true
        },
        {
            "question_number": 46,
            "expected": [
                "diminish",
                "supplanted by"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "diminish",
                "supplanted by"
            ],
            "model_explanation": "Final Answer: diminish, supplanted by",
            "model_response": "Final Answer: diminish, supplanted by",
            "runtime": 6.1,
            "correct": true
        },
        {
            "question_number": 47,
            "expected": [
                "inadequacies",
                "debunking"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "abstractions",
                "speculative"
            ],
            "model_explanation": "",
            "model_response": "abstractions, speculative",
            "runtime": 7.06,
            "correct": false
        },
        {
            "question_number": 48,
            "expected": [
                "liberalization",
                "abnegation"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "liberalization",
                "abnegation"
            ],
            "model_explanation": "Final Answer: liberalization, abnegation",
            "model_response": "Final Answer: liberalization, abnegation",
            "runtime": 4.47,
            "correct": true
        },
        {
            "question_number": 49,
            "expected": [
                "apologies",
                "awry in"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "apologies",
                "awry in"
            ],
            "model_explanation": "",
            "model_response": "apologies, awry in",
            "runtime": 6.44,
            "correct": true
        },
        {
            "question_number": 50,
            "expected": [
                "defensive",
                "an enhanced"
            ],
            "models_tried": [
                "gpt-4o"
            ],
            "model_used": "gpt-4o",
            "model_answer": [
                "defensive",
                "an enhanced"
            ],
            "model_explanation": "Final Answer: defensive, an enhanced",
            "model_response": "Final Answer: defensive, an enhanced",
            "runtime": 6.53,
            "correct": true
        }
    ]
}